
 <!DOCTYPE HTML>
<html lang="zh-cn">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta name="baidu_union_verify" content="d1952c66cf48912e21c18c7c581f382a">
  <meta name="360-site-verification" content="67fbcc5a67f4c65c057315b28fa0b2c8" />
<meta name="google-site-verification" content="2GzxQ0VtXwTSUdmGm6DzcmhTzM_I9QmzCb_pzpMzD88" />
  
    <title>Attention Mechanism in Deep Learning (2) | Sponge No.43</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Chan Zou">
    
    <meta name="description" content="PreviouslyIn the last post, we learnt basic idea of attention mechanism and the major difference between RCNN and it (AM combine context with current ">
    
    
    
    
    <link rel="alternate" href="atom.xml" title="Sponge No.43" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/ChanZou.ico">
    
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
    
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            var _bdId ='315597da49e436164821a87f3757ef4f';
             hm.src = "//hm.baidu.com/hm.js?" + _bdId;
             var s = document.getElementsByTagName("script")[0]; 
             s.parentNode.insertBefore(hm, s);
        })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
      
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Sponge No.43">Sponge No.43</a></h1>
				<a class="blog-motto"></a>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜單">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/tags">标签</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
					
                                            <form class="search" action=http://zhannei.baidu.com/cse/search target="_blank">
                                            <label>Search</label>
                                        <input name="s" type="hidden" value= null ><input type="text" name="q" size="30" placeholder="搜索"><br>
					
					</li>
				</ul>
                            </nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/05/29/2017052901/" title="Attention Mechanism in Deep Learning (2)" itemprop="url">Attention Mechanism in Deep Learning (2)</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://yoursite.com" title="Chan Zou">Chan Zou</a>
    </p>
  <p class="article-time">
    <time datetime="2017-05-30T00:00:00.000Z" itemprop="datePublished">2017-05-29</time>
    更新日期:<time datetime="2018-02-19T00:07:16.281Z" itemprop="dateModified">2018-02-18</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目錄</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Previously"><span class="toc-number">1.</span> <span class="toc-text">Previously</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention_Mechanism_in_NMT"><span class="toc-number">3.</span> <span class="toc-text">Attention Mechanism in NMT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Global_Mode_and_Local_Mode"><span class="toc-number">4.</span> <span class="toc-text">Global Mode and Local Mode</span></a></li></ol>
		</div>
		
		<h2 id="Previously">Previously</h2><p>In the last post, we learnt basic idea of attention mechanism and the major difference between RCNN and it (AM combine context with current image and RCNN focus on single image). Since adjacency in pixels already provide one ‘layer’ of context, attention mechanism should have more siginificant contribution in NLP.</p>
<h2 id="Introduction">Introduction</h2><p>Talking about language models, neural machine translation is a classical topic to discuss around. Classic NMT model follows the schema of encoder-decoder. Encoder encodes a source sentence into a fixed-length vector and decoder outputs a translation from it. It is not hard to tell that the bottleneck for its performance is its fixed-length presentation of arbitrary-length source sentence. The perfomance detoriorates rapidly as the length of an input increases.</p>
<h2 id="Attention_Mechanism_in_NMT">Attention Mechanism in NMT</h2><p>Intuitively, we can make use of internal states of every stages. But any award comes with a price. Under this schema, the model needs to learn align sentence elements explicitly, which is latently and can be naturally embedded in RNNs of LSTMs.</p>
<p>Basic idea behind <em>Neural Machine Translation by Jointly Learning to Align and Translate</em> is to learn weights to combine all stages’ state into a context vector. Now probability for each element’s translation is conditioned is conditioned on a distinct context vector. The question now is, how to compute the weights, especially when they vary at each decoding stage for each encoding stage.</p>
<p>The authors introduce an alignment model, a feedforward NN which is jointly trained with the whole system, to compute energy determining weight through a softmax function (to constrain weight sum to 1). In the experiment conducted, the NN is a single layer of perceptrons whose activation function is tanh. The inputs of the NN are linked encoder state and previous decoder state, illustrated by the annotation.<br><img src="http://ww1.sinaimg.cn/mw690/7446635dgy1fg2uzicj8ij20d807i75h.jpg" alt=""></p>
<h2 id="Global_Mode_and_Local_Mode">Global Mode and Local Mode</h2><p>The previous model relief performance detorioration brought by long sentence input. However, long sentence means more encoder states. So the computational cost grows linearly when sentence gets longer. <em>Effective Approaches to Attention-based Neural Machine Translation</em> propose several alternative model schemas to solve this problem.</p>
<p>We can easily tell that model introduced by the previous lecture is in global mode. Yet this lecture proposed several other functions computing the energy.<br><img src="http://ww1.sinaimg.cn/mw690/7446635dgy1fg2vy1fdcyj20d807i75h.jpg" alt=""></p>
<p>To localize attention, we need another function determining the center of it. In concrete details, the model first generates an aligned position $p_{t}$ for each target word $a_t$ time $t$. The context vector $c_t$ is then derived as a weighted average over the set of source hidden states within the window $[p_{t}−D, p_{t}+D]$ while $D$ is empirically selected. The function is:<br>$$p_{t} = S · sigmoid(v^T_{p}tanh(W_{p}h_{t}))$$<br>Since the assumption is that the closer one word to the center, the more important it could potentially be, the authors multiply original alignment function with a Gaussian distribution centered at $p_{t}$:<br>$$a_{t}(s) = align(h_{t},h_{s}) exp(-\frac{(s-p_{t})^2}{2\sigma^2}$$<br>Not that $\sigma$ is oftenly set to $D/2$ and $p_{t}$ is a real number while $s$ is an integer within the window centered at $p_{t}$.</p>
<p>While attentional decisions are made independently, there might be correlation between them. To make use of context in attentional decisions, this lecture proposed concatenating inputs with previous attentional vectors as illustrated below.<br><img src="http://ww1.sinaimg.cn/mw690/7446635dgy1fg2wvpm7m2j208506xt8z.jpg" alt=""><br>Under this schema, the deep NN spans both horizontally and vertically. Information of language content and alignment both flow within the network.</p>
  
	</div>
		<footer class="article-footer clearfix">




<div class="article-share" id="share">

  <div data-url="http://yoursite.com/2017/05/29/2017052901/" data-title="Attention Mechanism in Deep Learning (2) | Sponge No.43" data-tsina="1950770013" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/06/10/2017061001/" title="Attention Mechanism in Deep Learning">
  <strong>PREVIOUS:</strong><br/>
  <span>
  Attention Mechanism in Deep Learning</span>
</a>
</div>


<div class="next">
<a href="/2017/05/17/2017051701/"  title="Attention Mechanism in Deep Learning (1)">
 <strong>NEXT:</strong><br/> 
 <span>Attention Mechanism in Deep Learning (1)
</span>
</a>
</div>

</nav>

	
<section class="comment">
	
	<div class="ds-thread" data-title="Attention Mechanism in Deep Learning (2)" data-thread-key="2017052901" data-author-key="Chan Zou" data-url="http://yoursite.com/post/2017052901"></div>
	
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="顯示側邊欄"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目錄</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Previously"><span class="toc-number">1.</span> <span class="toc-text">Previously</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention_Mechanism_in_NMT"><span class="toc-number">3.</span> <span class="toc-text">Attention Mechanism in NMT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Global_Mode_and_Local_Mode"><span class="toc-number">4.</span> <span class="toc-text">Global Mode and Local Mode</span></a></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隱藏側邊欄"></a></div>
<aside class="clearfix">
<div id="authorInfo">
	
		<div class="author-logo"></div>		
	
	<div class="social-list" class="clearfix">
		
		<a href="http://weibo.com/1950770013" target="_blank" title="weibo"></a>
		
		
		
		<a href="https://github.com/chanzou" target="_blank" title="github"></a>
		
		
		
		<a href="https://zhihu.com/people/chanzou" target="_blank" title="zhihu"></a>
		
	</div>
</div>

  

  
<div class="tagslist">
	<p class="asidetitle">標簽</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Algorithm/" title="Algorithm">Algorithm<sup>2</sup></a></li>
		
			<li><a href="/tags/Data-Structure/" title="Data Structure">Data Structure<sup>2</sup></a></li>
		
			<li><a href="/tags/HDL/" title="HDL">HDL<sup>1</sup></a></li>
		
			<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>1</sup></a></li>
		
			<li><a href="/tags/MOOCs/" title="MOOCs">MOOCs<sup>7</sup></a></li>
		
			<li><a href="/tags/Neural-Network/" title="Neural Network">Neural Network<sup>7</sup></a></li>
		
			<li><a href="/tags/Statistics/" title="Statistics">Statistics<sup>3</sup></a></li>
		
			<li><a href="/tags/hexo/" title="hexo">hexo<sup>3</sup></a></li>
		
			<li><a href="/tags/spider/" title="spider">spider<sup>1</sup></a></li>
		
			<li><a href="/tags/spiders/" title="spiders">spiders<sup>2</sup></a></li>
		
			<li><a href="/tags/travel/" title="travel">travel<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情鏈接</p>
    <ul>
      <li><a href="http://hexo.io" target="_blank" title="Hexo">Hexo</a></li>
      <li><a href="http://gengbiao.me" target="_blank" title="coney">coney's Blog</a></li>
    </ul>
</div>


  <div class="rsspart">
	<a href="atom.xml" target="_blank" title="rss">RSS 訂閱</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
    
            <p class="copyright"> © 2018 
		
		<a href="http://yoursite.com" target="_blank" title="Chan Zou">Chan Zou</a>
		
            && Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> && Theme by <a href="http://gengbiao.me" target="_blank" title="coney">coney</a>
            </div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"chan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 









<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  </body>
</html>

