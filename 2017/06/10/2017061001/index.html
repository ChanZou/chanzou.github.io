
 <!DOCTYPE HTML>
<html lang="zh-cn">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta name="baidu_union_verify" content="d1952c66cf48912e21c18c7c581f382a">
  <meta name="360-site-verification" content="67fbcc5a67f4c65c057315b28fa0b2c8" />
<meta name="google-site-verification" content="2GzxQ0VtXwTSUdmGm6DzcmhTzM_I9QmzCb_pzpMzD88" />
  
    <title>Attention Mechanism in Deep Learning | Sponge No.43</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Chan Zou">
    
    <meta name="description" content="PreviouslyIn the last two posts, a visual model and a language model using attention mechanism are introduced. Today we will go through a blend of the">
    
    
    
    
    <link rel="alternate" href="atom.xml" title="Sponge No.43" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/ChanZou.ico">
    
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
    
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            var _bdId ='315597da49e436164821a87f3757ef4f';
             hm.src = "//hm.baidu.com/hm.js?" + _bdId;
             var s = document.getElementsByTagName("script")[0]; 
             s.parentNode.insertBefore(hm, s);
        })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
      
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Sponge No.43">Sponge No.43</a></h1>
				<a class="blog-motto"></a>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜單">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/tags">标签</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
					
                                            <form class="search" action=http://zhannei.baidu.com/cse/search target="_blank">
                                            <label>Search</label>
                                        <input name="s" type="hidden" value= null ><input type="text" name="q" size="30" placeholder="搜索"><br>
					
					</li>
				</ul>
                            </nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/06/10/2017061001/" title="Attention Mechanism in Deep Learning" itemprop="url">Attention Mechanism in Deep Learning</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://yoursite.com" title="Chan Zou">Chan Zou</a>
    </p>
  <p class="article-time">
    <time datetime="2017-06-11T02:00:00.000Z" itemprop="datePublished">2017-06-10</time>
    更新日期:<time datetime="2018-02-19T01:57:08.170Z" itemprop="dateModified">2018-02-18</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目錄</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Previously"><span class="toc-number">1.</span> <span class="toc-text">Previously</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Thoughts"><span class="toc-number">3.</span> <span class="toc-text">Thoughts</span></a></li></ol>
		</div>
		
		<h2 id="Previously">Previously</h2><p>In the last two posts, a visual model and a language model using attention mechanism are introduced. Today we will go through a blend of them, a model translating images into descriptions. And we will draw the period for this series by my own thoughts.</p>
<h2 id="Model">Model</h2><p>The paper <em>Show, Attend and Tell</em> adds attention mechanism to the existing reciepe of machine translation model. Traditional model feeds feature vectures of pictures into RNN (or LSTM)to generate scentences.<br><img src="http://ww1.sinaimg.cn/mw690/4b1b5b68gy1fgh4kv47w2j20r20vctd3.jpg" alt=""><br>It is reasonable trying to align regions of images with words of phrases of scentences. So the basic procedure will be divide images into regions, and use a attention layer (part) to decide how to compose a final feature vector.<br><img src="http://ww1.sinaimg.cn/mw690/4b1b5b68gy1fgh6bg8264j20si0j676p.jpg" alt=""><br>The schema is quite self-explaining. For $y_i$ extracted from sections $s_i$of an image, context of LSTM is added and combined by a tanh MLP layer. After a softmax layer served as normalization, the weight is then multiplied by the original $y_i$. Notice that the context is used, not the hidden state of LSTM.<br><img src="http://ww1.sinaimg.cn/mw690/4b1b5b68gy1fgh7d3bql9j20hk0em75h.jpg" alt="Structure of a LSTM"><br><img src="http://ww1.sinaimg.cn/mw690/4b1b5b68gy1fgh7ecqisgj20di06m3yw.jpg" alt="How LSTM evolve along time"><br>Here $i_t$, $f_t$, $c_t$, $o_t$, $h_t$ are the input, forget, memory, output and hidden state of the LSTM.</p>
<h2 id="Thoughts">Thoughts</h2><p>Actually, models apart from the RL visual model are looking at everything in detail before deciding where to focus. It is equivalent to have a memory space and computation resource needed increases rather than being saved. This is actually in contradiction with the intention of attention. RL method, however, is more like what human will act alikely. Admitting that attention mechanism is somewhat using memory, we will look into some memory addressing techniques.</p>
  
	</div>
		<footer class="article-footer clearfix">




<div class="article-share" id="share">

  <div data-url="http://yoursite.com/2017/06/10/2017061001/" data-title="Attention Mechanism in Deep Learning | Sponge No.43" data-tsina="1950770013" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/06/18/2017061801/" title="Linux Technologies Behind Docker">
  <strong>PREVIOUS:</strong><br/>
  <span>
  Linux Technologies Behind Docker</span>
</a>
</div>


<div class="next">
<a href="/2017/05/29/2017052901/"  title="Attention Mechanism in Deep Learning (2)">
 <strong>NEXT:</strong><br/> 
 <span>Attention Mechanism in Deep Learning (2)
</span>
</a>
</div>

</nav>

	
<section class="comment">
	
	<div class="ds-thread" data-title="Attention Mechanism in Deep Learning" data-thread-key="2017061001" data-author-key="Chan Zou" data-url="http://yoursite.com/post/2017061001"></div>
	
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="顯示側邊欄"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目錄</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Previously"><span class="toc-number">1.</span> <span class="toc-text">Previously</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Thoughts"><span class="toc-number">3.</span> <span class="toc-text">Thoughts</span></a></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隱藏側邊欄"></a></div>
<aside class="clearfix">
<div id="authorInfo">
	
		<div class="author-logo"></div>		
	
	<div class="social-list" class="clearfix">
		
		<a href="http://weibo.com/1950770013" target="_blank" title="weibo"></a>
		
		
		
		<a href="https://github.com/chanzou" target="_blank" title="github"></a>
		
		
		
		<a href="https://zhihu.com/people/chanzou" target="_blank" title="zhihu"></a>
		
	</div>
</div>

  

  
<div class="tagslist">
	<p class="asidetitle">標簽</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Algorithm/" title="Algorithm">Algorithm<sup>2</sup></a></li>
		
			<li><a href="/tags/Data-Structure/" title="Data Structure">Data Structure<sup>2</sup></a></li>
		
			<li><a href="/tags/HDL/" title="HDL">HDL<sup>1</sup></a></li>
		
			<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>1</sup></a></li>
		
			<li><a href="/tags/MOOCs/" title="MOOCs">MOOCs<sup>7</sup></a></li>
		
			<li><a href="/tags/Neural-Network/" title="Neural Network">Neural Network<sup>8</sup></a></li>
		
			<li><a href="/tags/Statistics/" title="Statistics">Statistics<sup>3</sup></a></li>
		
			<li><a href="/tags/hexo/" title="hexo">hexo<sup>3</sup></a></li>
		
			<li><a href="/tags/spider/" title="spider">spider<sup>1</sup></a></li>
		
			<li><a href="/tags/spiders/" title="spiders">spiders<sup>2</sup></a></li>
		
			<li><a href="/tags/travel/" title="travel">travel<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情鏈接</p>
    <ul>
      <li><a href="http://hexo.io" target="_blank" title="Hexo">Hexo</a></li>
      <li><a href="http://gengbiao.me" target="_blank" title="coney">coney's Blog</a></li>
    </ul>
</div>


  <div class="rsspart">
	<a href="atom.xml" target="_blank" title="rss">RSS 訂閱</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
    
            <p class="copyright"> © 2018 
		
		<a href="http://yoursite.com" target="_blank" title="Chan Zou">Chan Zou</a>
		
            && Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> && Theme by <a href="http://gengbiao.me" target="_blank" title="coney">coney</a>
            </div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"chan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 









<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  </body>
</html>

